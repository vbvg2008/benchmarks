objective:
	autoLR control (based on training history and validation history so far, decide next lr)



How does it work?
	training history + validation history (optional)  -> LRcontrolNet -> next lr



hypothetical outcome:
	* faster convergence (given the same objective)
	* better results(given the same computational resource)


impact if hypothetical outcome is true:
	* single model training becomes easier & faster
	* AutoML can discover more potential in each network architecure


================
Details:
================
1. what does the network predict?  here are some options:
	* classification, predict action class (each means a different action, say lr*1.1, lr*1.0, lr*0.9)
	* regression, (would be hard to collect the data, and there's nothing as "ideal lr")

2. what's the input format?
	* 1D time series (training), 1D time series(validation, optional)


3. the loss magnitude varries across different training, how to stndardize them?
	* divide by initial loss
	* divide by previous loss value

4. what network architecture to work on 1D?
	* network has to be small enough, because inferencing time is a constraint
	* 1D convolution? maybe
	* tabular dense connection
	* RNN based model

5. how often should we run the autoLR controller?
	* can't run too frequently because the inferencing will take some time
	* can't wait too long because LRController needs to control the training

6. how the


============
how


=================
Some limitations
==================
1. this AutoLR control should only work when the loss function is not changing throughout the training (in other words, can't use it on GAN stuff)

2. this AutoLR might/ might not work when the loss is calculated by intentional perturbation (like the one we did in fastcifar10)


===================


Experiment design (gathering data):

1. randomly choosing an initial learnign rate on log scale (between 1e-1 to 1e-4)
2. run the training for 100 steps, record losses for every step
3. save model
4. load model save in 3, perform following 3 decisions:
	a. lr*0.9 for 100 steps
	b. lr*1.0 for 100 steps
	c. lr*1.1 for 100 steps
5. evaluate the accuracy, generate ground truth
6. load model save in 3, randomly select an action, repeat 2-5 until training ends




